{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#tech-terrarium","title":"Tech-Terrarium","text":"<p>Documentation: https://foehammer82.github.io/tech-terrarium/</p> <p>Source Code: https://github.com/Foehammer82/tech-terrarium</p> <p>The Tech-Terrarium project is a compact hands-on tech stack simulation for learning and development. It comprises a full stack of interconnected services that are commonly used in Data Engineering and Software Engineering projects. Each service is set up to be deployed on docker using docker-compose. You will find each directory in this repo containing a docker-compose.yaml file that will start the given service and its dependencies.</p> <p>The Terrarium is designed such that each service can be run independently, though they also work in concert with each-other. For example, you can start the kafka service which will start a broker, schema registry, ksqldb server, an admin ui, and a few connectors. Two are configured to connect to the Terrarium's PostgreSQL and MongoDB services, while a third is designed to ingest dummy data from a data generator. So you can start just Kafka and see how it works and even develop against it. and you can start up the Postgres instance and see the kafka connector start sending data to Postgres. The diagram below illustrates the services in the Terrarium and how they interact with each other.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>I have often found myself digging through past projects to look up how different implementations were accomplished or to reference a past approach. I have also commonly found myself re-researching the same things over and over again throughout both personal and professional projects. Enter the <code>Tech-Terrarium</code>. This project is mostly for my own benefit to be able to quickly spin up, look at, and play with different approaches to Data Engineering and Software Engineering problems. And, if this helps someone else along the way, then that's a bonus!</p> <p>That said, if you do find yourself here and have questions, comments, feedback, or suggestions, please feel free to reach out or start create an <code>Issue</code> or <code>Pull Request</code>. And, this should go without saying, but please be respectful and considerate when making comments or suggestions.</p>"},{"location":"#project-architecture","title":"Project Architecture","text":"<pre><code>graph LR\n    U --&gt;|Sends Audit Data To| B\n    G[Feast] --&gt;|Offline Feature Store| E\n    A[FastAPI App] --&gt;|Sends Audit Data To| B[Kafka]\n    G --&gt;|Online Feature Store| V[Redis]\n    Q --&gt;|Tracks metadata of| E\n    Q --&gt;|Tracks metadata of| B\n    Q --&gt;|Tracks metadata of| D\n    B --&gt;|Streams Data To| D[PostgreSQL Sink Connector]\n    A --&gt;|Interacts With| E\n    D --&gt;|Streams Data Into| E[PostgreSQL]\n    Q --&gt;|Tracks metadata of| G\n    U --&gt;|Gets Features From| G\n    A --&gt;|Interacts With| U\n    H --&gt;|BI Data Analytics Against| I\n    T[MlFlow] --&gt;|Manages Lifecycle for| U(Model)\n    F[MongoDB Sink Connector] --&gt;|Streams Data Into| I\n    B --&gt;|Streams Data To| F\n    O --&gt;|Performs Data Operations On| I\n    Q[DataHub] --&gt;|Tracks metadata of| I[MongoDB]\n    Q --&gt;|Tracks metadata of| H\n    Q --&gt;|Tracks metadata of| O[Airflow]\n    O --&gt;|Publishes To| B\n    O --&gt;|Performs Data Operations On| E\n    H[Metabase] --&gt;|BI Data Analytics Against| E\n    Q --&gt;|Tracks metadata of| T\n    O --&gt;|Orchestrates| P\n    Q --&gt;|Tracks metadata of| P[DBT]\n    Q --&gt;|Tracks metadata of| A\n    P --&gt;|Operates on| E</code></pre>"},{"location":"#mvp-roadmap","title":"MVP Roadmap","text":"<ul> <li> set up a mkdocs site to document the terrarium</li> <li> add pre-commit checks to the project</li> <li> set up a Makefile to make it easier to run the services and start the Terrarium, parts of the terrarium.</li> <li> set up a project homepage using Homepage or something similar to make a   single point of entry for all the services in the Terrarium.</li> <li> Kafka Connectors<ul> <li>PostgreSQL Sink</li> <li>MongoDB Sink</li> </ul> </li> <li> setup metabase with some default dashboards for the terrarium</li> <li> implement DataHub to track metadata of the terrarium</li> <li> implement a DBT project to be orchestrated by airflow that does some operations on data within postgres<ul> <li>have it both generate and operate on data that it creates and uses and have it operate on datagen data being   produced from kafka</li> </ul> </li> <li> setup example python documentation examples to demonstrate interactions with the different services in the   terrarium</li> <li> build out the airflow instance with DAG's that perform scheduled operations on the rest of the services in the   terrarium</li> <li> implement openmetadata with the same setup as datahub</li> <li> create some endpoints in the FastAPI app that query topics using KSQL</li> <li> deploy a model using MLFlow and serve basic features to it from feast<ul> <li>deploy the model as a FastAPI app, and/or as an RPC service, that the main FastAPI app can interact with</li> </ul> </li> </ul>"},{"location":"#long-term-goals","title":"Long Term Goals","text":"<ul> <li> explore using arq for RPC<ul> <li>thinking of having the fastapi app be able to interact with another application (i.e. a model) using RPC with arq</li> <li>or look at other options (celery, apscheduler, etc.) to get an example together for developing distributed apps.</li> </ul> </li> <li> configure all exposed services to run through a Traefik load balancer<ul> <li>this would be a good exercise in setting up a reverse proxy and load balancer for the terrarium</li> </ul> </li> <li> set up a Spark server and explore that more<ul> <li>follow the quick start guide to get a feel for it and operate on local files (testing parquet, csv, json, avro,   etc.)</li> <li>see about setting up apache iceberg locally and having Spark operate on it.</li> </ul> </li> <li> configure to run everything on kubernetes with helm. the goal is to see if the whole stack can be deployed on 3   raspberry pi's using k3s.<ul> <li>write docs and instructions on setting up the hardware</li> <li>write docs and instructions on setting up the k3s cluster</li> <li>write docs and instructions for deploying the stack on the cluster</li> </ul> </li> </ul>"},{"location":"#projects-to-keep-an-eye-on","title":"Projects to Keep an Eye On","text":"<ul> <li>FastUI: A new way to build web application user interfaces defined by   declarative Python code... read more about it here</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#system-requirements","title":"System Requirements","text":"<ul> <li>CPU: at least 2 cores (4+ recommended)</li> <li>RAM: at least 16GB (32GB+ recommended)</li> <li>Storage: at least 64GB</li> </ul>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>This project assumes you have some basic knowledge of; git, docker, docker-compose, python, and SQL. However, if your objective is to learn these technologies, this project is a great way to get started.</p> <p>You will need to install:</p> <ul> <li>Docker</li> <li>Docker Compose</li> <li>Python (recommend 3.11)</li> <li>Poetry</li> </ul>"},{"location":"getting_started/#setup-python-environment","title":"Setup Python Environment","text":"<ol> <li> <p>Install Dependencies</p> <p><pre><code>poetry install --no-root\n</code></pre> 2. Setup Pre-Commit (optional) - If you plan to fork or contribute to this project, it is recommended to set up pre-commit hooks. - One of the pre-commit hooks automates keeping the project working correctly by exporting the poetry lock file   whenever the pyproject.toml file changes.</p> <pre><code>poetry run pre-commit install\n</code></pre> </li> </ol>"},{"location":"getting_started/#first-steps","title":"First Steps","text":"<p>TODO: build up the CLI app and then reference accessing it here</p> <p>information about all terrarium-admin CLI commands can be accessed by running:</p> <pre><code>poetry run admin --help\n</code></pre> <ol> <li>Start the <code>admin</code> services</li> <li> <p>once that the services are up, open your browser and navigate to http://localhost. you'll be    greeted with the Terrarium homepage that shows all the services available to you. though only the admin services are    running at this point.</p> </li> <li> <p>Start the services you'd like to use/explore/observe</p> </li> <li>And that's it! you're ready to start exploring the Terrarium.</li> <li>When you are done, you can stop all the services with:</li> <li>To clean everything up after you've shut down the services, you can run:</li> </ol>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you have the Terrarium up and running, you can start exploring the services and how they interact with each other. The Terrarium homepage is a great place to start as it provides a single point of entry to all the services in the Terrarium.</p> <p>You can find detailed information about each service, what it is, how to interact with it, etc. in the services section of the docs.</p>"},{"location":"references/","title":"References","text":""},{"location":"references/#fastapi-overview-of-concurency-and-async-await","title":"FastAPI Overview of \"Concurency and async / await\"","text":"<p>https://fastapi.tiangolo.com/async/</p> <p>Note</p> <p>This is a great overview written in the FastAPI docs explaining the <code>async def</code> syntax for path operation  functions and some background about asynchronous code, concurrency, and parallelism.</p>"},{"location":"references/#big-list-of-links-in-no-particular-order","title":"Big List of Links in no particular order","text":"<p>Abstract</p> <p>actually, one idea that might be worth poking at would be setting up a comment system or a way to submit links  that get would get sent to a queue (kafka topic, redis, rabbitmq, something...) and then use LLM's with web access  to summarize, categorize, tag, and then store to a database (or be sent to another queue for approval) and then diplay on a page or be the foundation of a link storing system. (might also be neat to have that create PR's against this very page)</p> <p>tl;dr: paste a link somewhere (or have a browser extension)</p> <p>Question</p> <p>this whole page is actually a bit outside the scope of the terrarium project and would make more sense to be it's  own project and deployment.</p> <ul> <li>Pythonize</li> <li>PyO3</li> <li> arq</li> <li> REST API's</li> <li> FastUI</li> </ul>"},{"location":"examples/kafka/","title":"Kafka","text":"In\u00a0[\u00a0]: Copied! <pre>print(\"Hello, Kafka!\")\n\n# TODO: add examples that show\n#       - pydantic producer and consumer\n#       - use of sync and async (with examples and notes on tradeoffs)\n#       - usage of avro and json schemas (and interacting with the schema registry)\n</pre> print(\"Hello, Kafka!\")  # TODO: add examples that show #       - pydantic producer and consumer #       - use of sync and async (with examples and notes on tradeoffs) #       - usage of avro and json schemas (and interacting with the schema registry)"},{"location":"notes/data_warehouse_vs_lake/","title":"Data Warehouse vs Data Lake","text":"<p>Choosing between a data warehouse and a data lake depends on your specific use case and data requirements:</p> <ol> <li> <p>Data Warehouse (e.g., Snowflake):</p> <ul> <li>Structured Data: Best for structured data with a predefined schema.</li> <li>Analytics and Reporting: Ideal for complex queries, business intelligence, and analytics.</li> <li>Performance: Optimized for read-heavy operations, providing fast query performance.</li> <li>Data Consistency: Ensures high data consistency and integrity, suitable for use cases where these are critical.</li> </ul> </li> <li> <p>Data Lake (e.g., Spark):</p> <ul> <li>Unstructured and Semi-structured Data: Suitable for storing a wide variety of data formats, including raw,   unstructured, and semi-structured data.</li> <li>Big Data Processing: Excellent for large-scale data processing and transformation tasks, including batch and   real-time processing.</li> <li>Flexibility: Provides greater flexibility for exploratory data analysis and machine learning workloads.</li> <li>Cost-Effective Storage: Typically offers more cost-effective storage for large volumes of data.</li> </ul> </li> </ol> <p>In summary, use a data warehouse like Snowflake for structured data, high-performance analytics, and business intelligence needs. Use a data lake with Spark for handling diverse data types, large-scale processing, and flexible data exploration and machine learning tasks.</p>"},{"location":"services/airflow/","title":"Airflow","text":"<p>Workflow scheduler that is used for orchestrating complex data pipelines. It is used for scheduling tasks/jobs/workflows and monitoring them.</p> <p>Alternatives: Luigi, Prefect, Dagster</p>"},{"location":"services/datahub/","title":"DataHub","text":"<p>Metadata management tool that is used for tracking metadata of data assets. It is used for discovering, understanding, and governing data assets.</p>"},{"location":"services/datahub/#alternatives","title":"Alternatives","text":""},{"location":"services/datahub/#openmetadata","title":"OpenMetadata","text":"<p>I actually set up an instance of OpenMetadata in the Terrarium, but it didn't have as strong of automated lineage as DataHub and really like DataHub's usage of kafka for streaming metadata. that said, you can spin up a local docker instance of OpenMetadata in 5-15 minutes by following their quick-start guide.</p>"},{"location":"services/dbt/","title":"DBT","text":"<p>Note</p> <p>I can not recommend DBT enough. It's a fantastic tool for transforming data and creating data models. My only  wish is that I had used it sooner on previous projects that it was LITERALLY designed to solve.</p> <p>Data transformation tool that is used for transforming data in the data warehouse. It is used for writing SQL queries to transform data and create data models.</p> <p>Alternatives: Apache Spark, Talend, Matillion</p>"},{"location":"services/dbt/#references","title":"References","text":"<ul> <li>Is DBT the right tool for my data transformations?</li> <li>The Spiritual Alignment of dbt + Airflow</li> <li>Orchestrating dbt with Airflow: A Step by Step Guide to Automating Data Pipelines \u2014 Part I</li> <li>Orchestrating dbt with Airflow: A Step by Step Guide to Automating Data Pipelines \u2014 Part II</li> </ul>"},{"location":"services/dbt/#airflow-orchestration","title":"Airflow Orchestration","text":"<p>TODO: review how DBT is orchestrated with Airflow</p>"},{"location":"services/fastapi_app/","title":"FastAPI App","text":"<p>Web framework for building APIs with Python. It is used for creating RESTful APIs with high performance and easy development.</p>"},{"location":"services/feast/","title":"Feast","text":"<p>Feature store that is used for managing and serving machine learning features. It is used for storing and serving features for training and serving machine learning models.</p>"},{"location":"services/great_expectations/","title":"Great Expectations","text":"<p>working on getting this implemented to play nice with DataHub, run in an airflow instance, and all-around just work has been a challenge to say the least. I'm almost hesitant to keep going, but there aren't many other options out there for data validation tooling like this. And based on looking through the source code i think what we have what we need to make this successful.</p> <p>If we can get this working, it might be worth considering building and releasing it as a plugin for Airflow to make it easier to integrate in future projects.</p>"},{"location":"services/great_expectations/#notes","title":"Notes:","text":"<p>For this to be successful in airflow, we must be able to create expectation DAG's easily by defining:</p> <ul> <li>the database we are running the expectations on (as an airflow connection id)</li> </ul> <p>then tasks should take in:</p> <ul> <li>table name</li> <li>the expectations to run on the table (which can be looked up from the expectations   catalog (https://greatexpectations.io/expectations/)</li> </ul> <p>THAT'S IT! that is how the user experience needs to be. My goal is to create a function that will take those values in leveraging pydantic models so that we can make it easy to see what expectations are doing and how they are configured without any risk of getting lost in the massive, MASSIVE, spaghetti code that is the great expectations codebase.</p>"},{"location":"services/kafka/","title":"Kafka","text":"<p>Distributed streaming platform that is used for building real-time data pipelines and streaming applications. It is used for publishing and subscribing to streams of records.</p> <p>In this stack, Kafka is being used to stream data between services and as a source for Flink.</p> <p>Alternatives: RabbitMQ, ActiveMQ, Pulsar</p>"},{"location":"services/kafka/#ksql-query-kafka-streams-with-sql","title":"KSQL - Query Kafka Streams with SQL","text":"<p>References:</p> <ul> <li>KSQL \u2014 Getting Started (Part 1)</li> <li>KSQL \u2014 Getting Started (Part 2)</li> </ul>"},{"location":"services/metabase/","title":"Metabase","text":"<p>Business intelligence tool that is used for visualizing and analyzing data. It is used for creating dashboards and reports.</p> <p>Alternatives: Tableau, Power BI, Looker</p>"},{"location":"services/mlflow/","title":"MlFlow","text":"<p>MlOps tool that is used for managing the end-to-end machine learning process. It is used for tracking experiments, packaging code, and deploying models.</p>"},{"location":"services/mongodb/","title":"MongoDB","text":"<p>NoSQL database that stores data in JSON-like documents with dynamic schemas. It is used to store unstructured data and</p> <p>Alternatives: CouchDB, Cassandra, HBase</p>"},{"location":"services/overview/","title":"Services Overview","text":"<p>TODO: give an overview of what the services are and how they are deployed and link to the getting started for how to use the terrarium</p> <p></p>"},{"location":"services/postgres/","title":"Postgres","text":"<p>Relational database that stores data in tables with rows and columns. It is used to store structured data and is commonly used in enterprise applications.</p> <p>Alternatives: MySQL, SQLite, Oracle</p>"},{"location":"services/postgres/#as-a-data-warehouse","title":"As a Data Warehouse","text":"<p>In this project we are simulating a data warehouse using Postgres. In production, or the \"real world\", an actual data warehouse would be used (i.e. Snowflake, Redshift, BigQuery, etc.). Data warehouses are used to store large amounts of data from various sources and are optimized for read-heavy workloads.</p>"},{"location":"services/scrubbed_services/","title":"Scrubbed Services","text":""},{"location":"services/scrubbed_services/#flink","title":"Flink","text":"<p>KSQL requires less expertise and has a smaller learning curve than Flink. Based on my explorations into Flink here i think it's an incredibly powerful tool, however I'm not sure the juice is worth the squeeze when we can much more quickly take advantage of KSQL. An article on Confluent's site really helped articulate this for me, and would recommend reading Flink vs Kafka Streams/ksqlDB: Comparing Stream Processing Tools before looking to implement or utilize Flink, and make sure it's the right tool for the job, and not a solution looking for a problem.</p>"},{"location":"services/scrubbed_services/#trino","title":"Trino","text":"<p>It ended up being more effort to implement than i expected. I think it's a great tool, but for the purposes of this project, it was overkill. I would recommend it for larger projects where you need to query multiple data sources, or need to query data in a distributed manner. May come back to it in the future, but for now, it's on the shelf.</p>"},{"location":"services/spark/","title":"Spark","text":"<p>Apache Spark is an open-source, distributed computing system used for big data processing and analytics. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It can handle both batch and real-time analytics and data processing workloads.</p> <p>Spark supports multiple programming languages such as Java, Python, and Scala and comes with built-in modules for SQL, streaming, machine learning, and graph processing. It can be run on Hadoop, standalone, or in the cloud and can access diverse data sources including HDFS, Cassandra, HBase, and S3.</p>"}]}